# ğŸ“˜ Word Embedding Similarity with GloVe

This project demonstrates how to use **pre-trained GloVe word embeddings** to compute **cosine similarity** between words. The goal is to find the top-N most similar words to a given query word.

---

### ğŸ“‚ Files

- `main.ipynb`: Jupyter notebook with complete code and explanations.
- `requirements.txt`: List of Python dependencies.
- `README.md`: Project overview.

---

### ğŸš€ Changes made after Interview

- âœ… Real word vectors using **GloVe** (via `gensim`)
- âœ… Handles **out-of-vocabulary (OOV)** words gracefully
- âœ… Computes **cosine similarity** between words
- âœ… Returns top-N most similar words
- âœ… Clean and readable output formatting
- âœ… Ready to run in any environment

---

### ğŸ“š Example Output

Top 3 most similar words to 'pear':
1. peach â€” Similarity: 0.7705
2. apples â€” Similarity: 0.6245
3. apple â€” Similarity: 0.5890

---

### ğŸ› ï¸ Requirements

```bash
pip install -r requirements.txt
